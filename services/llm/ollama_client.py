"""Ollama LLM –∫–ª–∏–µ–Ω—Ç - –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π async –æ–±—Ä–∞–±–æ—Ç–∫–æ–π."""

import asyncio
import logging
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor

try:
    import ollama
except ImportError:
    logging.error("‚ùå –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ ollama –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install ollama")
    ollama = None

from services.llm.base_client import BaseLLMClient
from models.base import BaseMessage, User

logger = logging.getLogger(__name__)

class OllamaClient(BaseLLMClient):
    """–ö–ª–∏–µ–Ω—Ç –¥–ª—è Ollama —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π async –æ–±—Ä–∞–±–æ—Ç–∫–æ–π."""
    
    def __init__(self, model_name: str, **kwargs):
        super().__init__(model_name, **kwargs)
        self.is_available = False
        self.active_model = None
        self._executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="ollama")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏
        self._check_availability()
    
    def _check_availability(self) -> None:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏."""
        if ollama is None:
            logger.warning("‚ö†Ô∏è Ollama –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
            return
            
        try:
            models = ollama.list()
            available_models = self._get_available_models_sync()
            
            # –ê–≤—Ç–æ–≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
            if self.model_name == "auto":
                self.active_model = self._select_best_model(available_models)
            else:
                self.active_model = self.model_name
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—ã–±—Ä–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å
            if self.active_model in available_models:
                self.is_available = True
                logger.info(f"‚úÖ Ollama –∫–ª–∏–µ–Ω—Ç –≥–æ—Ç–æ–≤ —Å –º–æ–¥–µ–ª—å—é {self.active_model}")
            else:
                logger.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {self.active_model} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {e}")
    
    async def initialize(self) -> bool:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è (–ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è)."""
        try:
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –≤ executor —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å event loop
            await asyncio.get_event_loop().run_in_executor(
                self._executor, self._check_availability
            )
            return self.is_available
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
            return False
    
    async def generate_response(
        self, 
        messages: List[BaseMessage], 
        user: User,
        **kwargs
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ Ollama –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ."""
        if not self.is_available or ollama is None:
            raise RuntimeError("Ollama –∫–ª–∏–µ–Ω—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            
        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç Ollama
            ollama_messages = self._convert_messages(messages, user)
            
            logger.debug(f"–û—Ç–ø—Ä–∞–≤–ª—è–µ–º {len(ollama_messages)} —Å–æ–æ–±—â–µ–Ω–∏–π –≤ Ollama")
            
            # –ü–†–ê–í–ò–õ–¨–ù–û: –≤—ã–ø–æ–ª–Ω—è–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤ –≤ executor
            response = await asyncio.get_event_loop().run_in_executor(
                self._executor, self._call_ollama_sync, ollama_messages
            )
            
            return response.strip()
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
            raise
    
    async def check_health(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ Ollama –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ."""
        if ollama is None:
            return False
            
        try:
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –≤ executor
            await asyncio.get_event_loop().run_in_executor(
                self._executor, ollama.list
            )
            return True
        except Exception:
            return False
    
    def get_available_models(self) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ (–º–æ–∂–µ—Ç –±—ã—Ç—å –≤—ã–∑–≤–∞–Ω —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ)."""
        return self._get_available_models_sync()
    
    def _get_available_models_sync(self) -> List[str]:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."""
        if ollama is None:
            return []
            
        try:
            models_response = ollama.list()
            
            if hasattr(models_response, 'models'):
                models_list = models_response.models
            elif isinstance(models_response, dict):
                models_list = models_response.get('models', [])
            else:
                return []
            
            models = []
            for model in models_list:
                if hasattr(model, 'model'):
                    models.append(model.model)
                elif isinstance(model, dict):
                    models.append(model.get('model', ''))
                elif hasattr(model, 'name'):
                    models.append(model.name)
            
            return [m for m in models if m]
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {e}")
            return []
    
    def _select_best_model(self, available: List[str]) -> str:
        """–í—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é –º–æ–¥–µ–ª—å."""
        preferred = [
            'llama3.2:3b', 'llama3.2:1b', 'llama3.2',
            'mistral:7b', 'mistral', 'qwen2.5:7b'
        ]
        
        for pref in preferred:
            for model in available:
                if pref.lower() in model.lower():
                    logger.info(f"üéØ –í—ã–±—Ä–∞–Ω–∞ –º–æ–¥–µ–ª—å: {model}")
                    return model
        
        if available:
            logger.info(f"üéØ –í—ã–±—Ä–∞–Ω–∞ –ø–µ—Ä–≤–∞—è –¥–æ—Å—Ç—É–ø–Ω–∞—è –º–æ–¥–µ–ª—å: {available[0]}")
            return available[0]
        
        logger.warning("‚ö†Ô∏è –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
        return "llama3.2:3b"
    
    def _convert_messages(self, messages: List[BaseMessage], user: User) -> List[Dict]:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç Ollama."""
        ollama_messages = []
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ –¥–ª—è —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
        try:
            from core.registry import registry
            character_service = registry.get('character', None)
        except Exception:
            character_service = None
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        if character_service and hasattr(character_service, 'get_system_prompt'):
            system_prompt = character_service.get_system_prompt(user)
            ollama_messages.append({
                "role": "system",
                "content": system_prompt
            })
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (—Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤)
        for msg in messages[-5:]:
            ollama_messages.append({
                "role": msg.role.value,
                "content": msg.content
            })
        
        return ollama_messages
    
    def _call_ollama_sync(self, messages: List[Dict]) -> str:
        """–°–ò–ù–•–†–û–ù–ù–´–ô –≤—ã–∑–æ–≤ Ollama API - –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ."""
        try:
            logger.debug(f"–í—ã–∑–æ–≤ Ollama —Å –º–æ–¥–µ–ª—å—é {self.active_model}")
            
            # –ü—Ä–æ–±—É–µ–º chat API (–ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã–π —Å–ø–æ—Å–æ–±)
            response = ollama.chat(
                model=self.active_model,
                messages=messages,
                options={
                    'temperature': self.config.get('temperature', 0.7),
                    'num_predict': self.config.get('max_tokens', 200),
                    'top_p': 0.9,
                    'top_k': 40
                }
            )
            
            if 'message' in response and 'content' in response['message']:
                return response['message']['content']
            else:
                logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞: {response}")
                return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞."
            
        except Exception as chat_error:
            logger.warning(f"Chat API –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {chat_error}, –ø—Ä–æ–±—É–µ–º generate API")
            
            try:
                # Fallback –Ω–∞ generate API
                prompt = self._messages_to_prompt(messages)
                response = ollama.generate(
                    model=self.active_model,
                    prompt=prompt,
                    options={
                        'temperature': self.config.get('temperature', 0.7),
                        'num_predict': self.config.get('max_tokens', 200)
                    }
                )
                return response['response']
                
            except Exception as generate_error:
                logger.error(f"Generate API —Ç–æ–∂–µ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {generate_error}")
                raise
    
    def _messages_to_prompt(self, messages: List[Dict]) -> str:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –ø—Ä–æ–º–ø—Ç –¥–ª—è generate API."""
        prompt_parts = []
        
        for msg in messages:
            role = msg['role']
            content = msg['content']
            
            if role == 'system':
                prompt_parts.append(f"–°–∏—Å—Ç–µ–º–∞: {content}")
            elif role == 'user':
                prompt_parts.append(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {content}")
            elif role == 'assistant':
                prompt_parts.append(f"–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç: {content}")
        
        prompt_parts.append("–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:")
        return "\n".join(prompt_parts)
    
    async def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤."""
        logger.info("üßπ –û—á–∏—Å—Ç–∫–∞ Ollama –∫–ª–∏–µ–Ω—Ç–∞...")
        if hasattr(self, '_executor'):
            self._executor.shutdown(wait=True)
        logger.debug("‚úÖ Ollama –∫–ª–∏–µ–Ω—Ç –æ—á–∏—â–µ–Ω")
    
    def __del__(self):
        """–î–µ—Å—Ç—Ä—É–∫—Ç–æ—Ä –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ executor."""
        if hasattr(self, '_executor'):
            try:
                self._executor.shutdown(wait=False)
            except Exception:
                pass