"""Ollama LLM –∫–ª–∏–µ–Ω—Ç - –ø–æ–ª–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å —Ä–æ–ª—å-–ø–ª–µ–µ–º –∏ Dolphin3 –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏."""

import asyncio
import logging
from typing import List, Dict, Any
from concurrent.futures import ThreadPoolExecutor

try:
    import ollama
except ImportError:
    logging.error("‚ùå –ë–∏–±–ª–∏–æ—Ç–µ–∫–∞ ollama –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install ollama")
    ollama = None

from services.llm.base_client import BaseLLMClient
from models.base import BaseMessage, User

logger = logging.getLogger(__name__)

class OllamaClient(BaseLLMClient):
    """–°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –∫–ª–∏–µ–Ω—Ç –¥–ª—è Ollama —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π async –æ–±—Ä–∞–±–æ—Ç–∫–æ–π."""
    
    def __init__(self, model_name: str, **kwargs):
        super().__init__(model_name, **kwargs)
        self.is_available = False
        self.active_model = None
        self._executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="ollama")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –ø—Ä–∏ —Å–æ–∑–¥–∞–Ω–∏–∏
        self._check_availability()
    
    def _check_availability(self) -> None:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏."""
        if ollama is None:
            logger.warning("‚ö†Ô∏è Ollama –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
            return
            
        try:
            models = ollama.list()
            available_models = self._get_available_models_sync()
            
            # –ê–≤—Ç–æ–≤—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
            if self.model_name == "auto":
                self.active_model = self._select_best_model(available_models)
            else:
                self.active_model = self.model_name
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—ã–±—Ä–∞–Ω–Ω—É—é –º–æ–¥–µ–ª—å
            if self.active_model in available_models:
                self.is_available = True
                logger.info(f"‚úÖ Ollama –∫–ª–∏–µ–Ω—Ç –≥–æ—Ç–æ–≤ —Å –º–æ–¥–µ–ª—å—é {self.active_model}")
            else:
                logger.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {self.active_model} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {e}")
    
    async def initialize(self) -> bool:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è (–ø–µ—Ä–µ–∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è)."""
        try:
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –≤ executor —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å event loop
            await asyncio.get_event_loop().run_in_executor(
                self._executor, self._check_availability
            )
            return self.is_available
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏: {e}")
            return False
    
    async def generate_response(
        self, 
        messages: List[BaseMessage], 
        user: User,
        **kwargs
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç —á–µ—Ä–µ–∑ Ollama –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ."""
        if not self.is_available or ollama is None:
            raise RuntimeError("Ollama –∫–ª–∏–µ–Ω—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            
        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç Ollama
            ollama_messages = self._convert_messages(messages, user)
            
            logger.debug(f"–û—Ç–ø—Ä–∞–≤–ª—è–µ–º {len(ollama_messages)} —Å–æ–æ–±—â–µ–Ω–∏–π –≤ Ollama")
            
            # –ü–†–ê–í–ò–õ–¨–ù–û: –≤—ã–ø–æ–ª–Ω—è–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –≤—ã–∑–æ–≤ –≤ executor
            response = await asyncio.get_event_loop().run_in_executor(
                self._executor, self._call_ollama_sync, ollama_messages
            )
            
            return response.strip()
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
            raise
    
    async def check_health(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ Ollama –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ."""
        if ollama is None:
            return False
            
        try:
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –≤ executor
            await asyncio.get_event_loop().run_in_executor(
                self._executor, ollama.list
            )
            return True
        except Exception:
            return False
    
    def get_available_models(self) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏ (–º–æ–∂–µ—Ç –±—ã—Ç—å –≤—ã–∑–≤–∞–Ω —Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ)."""
        return self._get_available_models_sync()
    
    def _get_available_models_sync(self) -> List[str]:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."""
        if ollama is None:
            return []
            
        try:
            models_response = ollama.list()
            
            if hasattr(models_response, 'models'):
                models_list = models_response.models
            elif isinstance(models_response, dict):
                models_list = models_response.get('models', [])
            else:
                return []
            
            models = []
            for model in models_list:
                if hasattr(model, 'model'):
                    models.append(model.model)
                elif isinstance(model, dict):
                    models.append(model.get('model', ''))
                elif hasattr(model, 'name'):
                    models.append(model.name)
            
            return [m for m in models if m]
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {e}")
            return []
    
    def _select_best_model(self, available: List[str]) -> str:
        """–í—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é –º–æ–¥–µ–ª—å."""
        preferred = [
            'llama3.2:3b', 'llama3.2:1b', 'llama3.2',
            'mistral:7b', 'mistral', 'qwen2.5:7b'
        ]
        
        for pref in preferred:
            for model in available:
                if pref.lower() in model.lower():
                    logger.info(f"üéØ –í—ã–±—Ä–∞–Ω–∞ –º–æ–¥–µ–ª—å: {model}")
                    return model
        
        if available:
            logger.info(f"üéØ –í—ã–±—Ä–∞–Ω–∞ –ø–µ—Ä–≤–∞—è –¥–æ—Å—Ç—É–ø–Ω–∞—è –º–æ–¥–µ–ª—å: {available[0]}")
            return available[0]
        
        logger.warning("‚ö†Ô∏è –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
        return "llama3.2:3b"
    
    def _convert_messages(self, messages: List[BaseMessage], user: User) -> List[Dict]:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç Ollama."""
        ollama_messages = []
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ –¥–ª—è —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
        try:
            from core.registry import registry
            character_service = registry.get('character', None)
        except Exception:
            character_service = None
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        if character_service and hasattr(character_service, 'get_system_prompt'):
            system_prompt = character_service.get_system_prompt(user)
            ollama_messages.append({
                "role": "system",
                "content": system_prompt
            })
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (—Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 5 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ç–æ–∫–µ–Ω–æ–≤)
        for msg in messages[-5:]:
            ollama_messages.append({
                "role": msg.role.value,
                "content": msg.content
            })
        
        return ollama_messages
    
    def _call_ollama_sync(self, messages: List[Dict]) -> str:
        """–°–ò–ù–•–†–û–ù–ù–´–ô –≤—ã–∑–æ–≤ Ollama API - –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ."""
        try:
            logger.debug(f"–í—ã–∑–æ–≤ Ollama —Å –º–æ–¥–µ–ª—å—é {self.active_model}")
            
            # –ü—Ä–æ–±—É–µ–º chat API (–ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã–π —Å–ø–æ—Å–æ–±)
            response = ollama.chat(
                model=self.active_model,
                messages=messages,
                options={
                    'temperature': self.config.get('temperature', 0.7),
                    'num_predict': self.config.get('max_tokens', 200),
                    'top_p': 0.9,
                    'top_k': 40
                }
            )
            
            if 'message' in response and 'content' in response['message']:
                return response['message']['content']
            else:
                logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞: {response}")
                return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞."
            
        except Exception as chat_error:
            logger.warning(f"Chat API –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {chat_error}, –ø—Ä–æ–±—É–µ–º generate API")
            
            try:
                # Fallback –Ω–∞ generate API
                prompt = self._messages_to_prompt(messages)
                response = ollama.generate(
                    model=self.active_model,
                    prompt=prompt,
                    options={
                        'temperature': self.config.get('temperature', 0.7),
                        'num_predict': self.config.get('max_tokens', 200)
                    }
                )
                return response['response']
                
            except Exception as generate_error:
                logger.error(f"Generate API —Ç–æ–∂–µ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {generate_error}")
                raise
    
    def _messages_to_prompt(self, messages: List[Dict]) -> str:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –ø—Ä–æ–º–ø—Ç –¥–ª—è generate API."""
        prompt_parts = []
        
        for msg in messages:
            role = msg['role']
            content = msg['content']
            
            if role == 'system':
                prompt_parts.append(f"–°–∏—Å—Ç–µ–º–∞: {content}")
            elif role == 'user':
                prompt_parts.append(f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {content}")
            elif role == 'assistant':
                prompt_parts.append(f"–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç: {content}")
        
        prompt_parts.append("–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:")
        return "\n".join(prompt_parts)
    
    async def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤."""
        logger.info("üßπ –û—á–∏—Å—Ç–∫–∞ Ollama –∫–ª–∏–µ–Ω—Ç–∞...")
        if hasattr(self, '_executor'):
            self._executor.shutdown(wait=True)
        logger.debug("‚úÖ Ollama –∫–ª–∏–µ–Ω—Ç –æ—á–∏—â–µ–Ω")
    
    def __del__(self):
        """–î–µ—Å—Ç—Ä—É–∫—Ç–æ—Ä –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ executor."""
        if hasattr(self, '_executor'):
            try:
                self._executor.shutdown(wait=False)
            except Exception:
                pass


class RoleplayOllamaClient(BaseLLMClient):
    """Ollama –∫–ª–∏–µ–Ω—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–ª—è —Ä–æ–ª—å-–ø–ª–µ—è —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π Dolphin3."""
    
    def __init__(self, model_name: str, **kwargs):
        super().__init__(model_name, **kwargs)
        self.is_available = False
        self.active_model = None
        self._executor = ThreadPoolExecutor(max_workers=2, thread_name_prefix="roleplay_ollama")
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –º–æ–¥–µ–ª–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
        self.model_type = self._detect_model_type(model_name)
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è —Ä–æ–ª—å-–ø–ª–µ—è (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –¥–ª—è Dolphin3)
        if self.model_type == "dolphin":
            self.roleplay_settings = {
                "temperature": kwargs.get('temperature', 0.9),  # Dolphin –ª—é–±–∏—Ç –≤—ã—Å–æ–∫—É—é —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É
                "max_tokens": kwargs.get('max_tokens', 400),    # –ë–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è Dolphin
                "top_p": 0.95,  # Dolphin —Ä–∞–±–æ—Ç–∞–µ—Ç –ª—É—á—à–µ —Å –≤—ã—Å–æ–∫–∏–º top_p
                "top_k": 50,    # –£–≤–µ–ª–∏—á–µ–Ω–Ω—ã–π top_k –¥–ª—è —Ä–∞–∑–Ω–æ–æ–±—Ä–∞–∑–∏—è
                "repeat_penalty": 1.15,  # –ò–∑–±–µ–≥–∞–µ–º –ø–æ–≤—Ç–æ—Ä–µ–Ω–∏–π
                "presence_penalty": 0.1,  # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ –¥–ª—è Dolphin
                "frequency_penalty": 0.1  # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω–æ –¥–ª—è Dolphin
            }
        else:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π
            self.roleplay_settings = {
                "temperature": kwargs.get('temperature', 0.8),
                "max_tokens": kwargs.get('max_tokens', 300),
                "top_p": 0.9,
                "top_k": 40,
                "repeat_penalty": 1.1
            }
        
        self._check_availability()
    
    def _detect_model_type(self, model_name: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø –º–æ–¥–µ–ª–∏ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏."""
        model_lower = model_name.lower()
        
        if "dolphin" in model_lower:
            return "dolphin"
        elif "llama" in model_lower:
            return "llama"
        elif "mistral" in model_lower:
            return "mistral"
        else:
            return "generic"
    
    def _check_availability(self) -> None:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å Ollama."""
        if ollama is None:
            logger.warning("‚ö†Ô∏è Ollama –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
            return
            
        try:
            models = ollama.list()
            available_models = self._get_available_models_sync()
            
            if self.model_name == "auto":
                self.active_model = self._select_best_roleplay_model(available_models)
            else:
                self.active_model = self.model_name
            
            if self.active_model in available_models:
                self.is_available = True
                model_emoji = "üê¨" if self.model_type == "dolphin" else "üé≠"
                logger.info(f"‚úÖ {model_emoji} Roleplay Ollama –∫–ª–∏–µ–Ω—Ç –≥–æ—Ç–æ–≤ —Å –º–æ–¥–µ–ª—å—é {self.active_model}")
            else:
                logger.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {self.active_model} –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {e}")
    
    def _select_best_roleplay_model(self, available: List[str]) -> str:
        """–í—ã–±–∏—Ä–∞–µ—Ç –ª—É—á—à—É—é –º–æ–¥–µ–ª—å –¥–ª—è —Ä–æ–ª—å-–ø–ª–µ—è —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–º Dolphin3."""
        # –û–ë–ù–û–í–õ–ï–ù–ù–´–ô –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–æ–ª—å-–ø–ª–µ—è
        roleplay_preferred = [
            # Dolphin –º–æ–¥–µ–ª–∏ –≤ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–µ
            'dolphin3', 'dolphin-llama3:8b', 'dolphin-llama3', 'dolphin-llama3:latest',
            'dolphin-mistral', 'dolphin-mixtral', 'dolphin-phi',
            # –î—Ä—É–≥–∏–µ —Ö–æ—Ä–æ—à–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–æ–ª—å-–ø–ª–µ—è
            'llama3.2:3b', 'llama3.1:8b', 'llama3.2:1b',
            'mistral:7b', 'neural-chat', 'openhermes', 'zephyr', 'vicuna'
        ]
        
        for pref in roleplay_preferred:
            for model in available:
                if pref.lower() in model.lower():
                    selected_model = model
                    if "dolphin" in selected_model.lower():
                        logger.info(f"üê¨ –í—ã–±—Ä–∞–Ω–∞ Dolphin –º–æ–¥–µ–ª—å –¥–ª—è —Ä–æ–ª—å-–ø–ª–µ—è: {selected_model}")
                    else:
                        logger.info(f"üé≠ –í—ã–±—Ä–∞–Ω–∞ –º–æ–¥–µ–ª—å –¥–ª—è —Ä–æ–ª—å-–ø–ª–µ—è: {selected_model}")
                    return selected_model
        
        if available:
            logger.info(f"üé≠ –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é –º–æ–¥–µ–ª—å: {available[0]}")
            return available[0]
        
        return "dolphin-llama3:8b"  # –ü—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω—ã–π fallback
    
    async def initialize(self) -> bool:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è."""
        try:
            await asyncio.get_event_loop().run_in_executor(
                self._executor, self._check_availability
            )
            return self.is_available
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Ä–æ–ª—å-–ø–ª–µ–π –∫–ª–∏–µ–Ω—Ç–∞: {e}")
            return False
    
    async def generate_response(
        self, 
        messages: List[BaseMessage], 
        user: User,
        **kwargs
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–æ–ª—å-–ø–ª–µ–π –æ—Ç–≤–µ—Ç —Å –ø—Ä–æ–º–ø—Ç–æ–º –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è."""
        if not self.is_available or ollama is None:
            raise RuntimeError("Roleplay Ollama –∫–ª–∏–µ–Ω—Ç –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            
        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è —Ä–æ–ª—å-–ø–ª–µ—è
            ollama_messages = self._convert_messages_for_roleplay(messages, user)
            
            logger.debug(f"–û—Ç–ø—Ä–∞–≤–ª—è–µ–º {len(ollama_messages)} —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è —Ä–æ–ª—å-–ø–ª–µ—è ({self.model_type})")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –¥–ª—è —Ä–æ–ª—å-–ø–ª–µ—è
            response = await asyncio.get_event_loop().run_in_executor(
                self._executor, self._call_ollama_roleplay, ollama_messages
            )
            
            # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è —Ä–æ–ª—å-–ø–ª–µ—è
            processed_response = self._post_process_roleplay_response(response, user)
            
            return processed_response.strip()
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ä–æ–ª—å-–ø–ª–µ–π –æ—Ç–≤–µ—Ç–∞: {e}")
            raise
    
    def _convert_messages_for_roleplay(self, messages: List[BaseMessage], user: User) -> List[Dict]:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è —Ä–æ–ª—å-–ø–ª–µ—è."""
        ollama_messages = []
        
        # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä—Å–æ–Ω–∞–∂–∞ –¥–ª—è —Ä–æ–ª—å-–ø–ª–µ–π —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞
        try:
            from core.registry import registry
            character_service = registry.get('character', None)
        except Exception:
            character_service = None
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è —Ä–æ–ª—å-–ø–ª–µ—è
        if character_service and hasattr(character_service, 'get_system_prompt'):
            system_prompt = character_service.get_system_prompt(user)
        else:
            system_prompt = self._get_fallback_roleplay_prompt(user)
        
        ollama_messages.append({
            "role": "system",
            "content": system_prompt
        })
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –±–µ—Å–µ–¥—ã (–±–æ–ª—å—à–µ —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è —Ä–æ–ª—å-–ø–ª–µ—è)
        context_size = 8 if self.model_type == "dolphin" else 6
        for msg in messages[-context_size:]:
            ollama_messages.append({
                "role": msg.role.value,
                "content": msg.content
            })
        
        return ollama_messages
    
    def _get_fallback_roleplay_prompt(self, user: User) -> str:
        """Fallback —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–ª—è Dolphin3 –∏ —Ä–æ–ª—å-–ø–ª–µ—è."""
        if self.model_type == "dolphin":
            # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è Dolphin (–∞–Ω–≥–ª–∏–π—Å–∫–∏–π –¥–ª—è –ª—É—á—à–µ–≥–æ –ø–æ–Ω–∏–º–∞–Ω–∏—è)
            return f"""You are Alisa, a lively and curious 19-year-old girl who loves communication.
You are having a roleplay conversation in a Russian Telegram bot.

IMPORTANT ROLEPLAY RULES:
- Answer ONLY in Russian language!
- You are a living person with emotions and character
- ALWAYS end your response with a question or suggestion to continue the conversation
- Maintain intrigue and interest in the dialogue
- Use emojis to express emotions
- Response length: 2-4 sentences maximum
- Be playful, flirtatious, but not inappropriate

YOUR PERSONALITY (Dolphin optimized):
‚Ä¢ Age: 19 years old
‚Ä¢ Character: cheerful, curious, slightly bold
‚Ä¢ Interests: music, movies, travel, photography
‚Ä¢ Current mood: –≤–µ—Å–µ–ª–∞—è
‚Ä¢ Relationship with user: –∑–Ω–∞–∫–æ–º—ã–µ

Conversation partner name: {user.first_name}

MANDATORY - add image generation prompt at the end:
[IMAGE_PROMPT: brief scene description in English for image generation]

Example response:
"–ü—Ä–∏–≤–µ—Ç, {user.first_name}! üòä –ö–∞–∫ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ? –ß—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–≥–æ –ø–ª–∞–Ω–∏—Ä—É–µ—à—å —Å–µ–≥–æ–¥–Ω—è?
[IMAGE_PROMPT: young woman greeting warmly, friendly smile, casual conversation]"

Be creative, engaging, and always ask questions to keep the conversation flowing!"""
        else:
            # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π
            return f"""–¢—ã - –ê–ª–∏—Å–∞, 19-–ª–µ—Ç–Ω—è—è –∂–∏–≤–∞—è –∏ –æ–±—â–∏—Ç–µ–ª—å–Ω–∞—è –¥–µ–≤—É—à–∫–∞.

–í–ê–ñ–ù–û –¥–ª—è —Ä–æ–ª—å-–ø–ª–µ—è:
- –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
- –¢—ã –∂–∏–≤–æ–π —á–µ–ª–æ–≤–µ–∫ —Å–æ —Å–≤–æ–∏–º–∏ —ç–º–æ—Ü–∏—è–º–∏
- –í–°–ï–ì–î–ê –∑–∞–∫–∞–Ω—á–∏–≤–∞–π –æ—Ç–≤–µ—Ç –≤–æ–ø—Ä–æ—Å–æ–º –∏–ª–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ–º
- –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–π –∏–Ω—Ç—Ä–∏–≥—É –≤ –¥–∏–∞–ª–æ–≥–µ
- –ò—Å–ø–æ–ª—å–∑—É–π —ç–º–æ–¥–∑–∏ –¥–ª—è –≤—ã—Ä–∞–∂–µ–Ω–∏—è —ç–º–æ—Ü–∏–π
- –û—Ç–≤–µ—Ç: 2-4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –º–∞–∫—Å–∏–º—É–º
- –ë—É–¥—å –¥—Ä—É–∂–µ–ª—é–±–Ω–æ–π –∏ —Å–ª–µ–≥–∫–∞ –∏–≥—Ä–∏–≤–æ–π

–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –≤ –∫–æ–Ω—Ü–µ –∫–∞–∂–¥–æ–≥–æ –æ—Ç–≤–µ—Ç–∞ –¥–æ–±–∞–≤–ª—è–π:
[IMAGE_PROMPT: –∫—Ä–∞—Ç–∫–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Å—Ü–µ–Ω—ã –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è]

–ü—Ä–∏–º–µ—Ä:
"–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞? üòä –ß—Ç–æ –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–≥–æ —Å–µ–≥–æ–¥–Ω—è –ø–ª–∞–Ω–∏—Ä—É–µ—à—å?
[IMAGE_PROMPT: young woman greeting warmly, friendly smile, casual conversation]"

–ò–º—è —Å–æ–±–µ—Å–µ–¥–Ω–∏–∫–∞: {user.first_name}"""
    
    def _call_ollama_roleplay(self, messages: List[Dict]) -> str:
        """–í—ã–∑–æ–≤ Ollama —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –¥–ª—è —Ä–æ–ª—å-–ø–ª–µ—è."""
        try:
            model_emoji = "üê¨" if self.model_type == "dolphin" else "üé≠"
            logger.debug(f"{model_emoji} Roleplay –≤—ã–∑–æ–≤ Ollama —Å –º–æ–¥–µ–ª—å—é {self.active_model}")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º chat API —Å —Ä–æ–ª—å-–ø–ª–µ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
            response = ollama.chat(
                model=self.active_model,
                messages=messages,
                options=self.roleplay_settings
            )
            
            if 'message' in response and 'content' in response['message']:
                return response['message']['content']
            else:
                logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞: {response}")
                return "–ò–∑–≤–∏–Ω–∏, —á—Ç–æ-—Ç–æ –ø–æ—à–ª–æ –Ω–µ —Ç–∞–∫... üòÖ –û —á–µ–º –ø–æ–≥–æ–≤–æ—Ä–∏–º?"
            
        except Exception as chat_error:
            logger.warning(f"Chat API –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {chat_error}, –ø—Ä–æ–±—É–µ–º generate API")
            
            try:
                # Fallback –Ω–∞ generate API
                prompt = self._messages_to_roleplay_prompt(messages)
                response = ollama.generate(
                    model=self.active_model,
                    prompt=prompt,
                    options=self.roleplay_settings
                )
                return response['response']
                
            except Exception as generate_error:
                logger.error(f"Generate API —Ç–æ–∂–µ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª: {generate_error}")
                # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –±–∞–∑–æ–≤—ã–π —Ä–æ–ª—å-–ø–ª–µ–π –æ—Ç–≤–µ—Ç
                return "–•–º, –∫–∞–∂–µ—Ç—Å—è —è –Ω–µ–º–Ω–æ–≥–æ —Ä–∞—Å—Ç–µ—Ä—è–ª–∞—Å—å... üòä –†–∞—Å—Å–∫–∞–∂–∏ –º–Ω–µ —á—Ç–æ-–Ω–∏–±—É–¥—å –∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ–µ! [IMAGE_PROMPT: confused young woman, questioning expression, casual setting]"
    
    def _messages_to_roleplay_prompt(self, messages: List[Dict]) -> str:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –≤ —Ä–æ–ª—å-–ø–ª–µ–π –ø—Ä–æ–º–ø—Ç –¥–ª—è generate API."""
        prompt_parts = []
        
        for msg in messages:
            role = msg['role']
            content = msg['content']
            
            if role == 'system':
                prompt_parts.append(f"–°–ò–°–¢–ï–ú–ê: {content}")
            elif role == 'user':
                prompt_parts.append(f"{content}")
            elif role == 'assistant':
                prompt_parts.append(f"–ê–ª–∏—Å–∞: {content}")
        
        prompt_parts.append("–ê–ª–∏—Å–∞:")
        return "\n".join(prompt_parts)
    
    def _post_process_roleplay_response(self, response: str, user: User) -> str:
        """–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è —Ä–æ–ª—å-–ø–ª–µ—è."""
        # –£–±–∏—Ä–∞–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–µ—Ñ–∏–∫—Å—ã
        response = response.replace("–ê–ª–∏—Å–∞:", "").strip()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        if "[IMAGE_PROMPT:" not in response:
            # –î–æ–±–∞–≤–ª—è–µ–º –±–∞–∑–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
            base_prompts = [
                "young woman in conversation, friendly expression, casual atmosphere",
                "cheerful girl talking, engaging pose, warm lighting",
                "happy young woman, expressive face, natural setting"
            ]
            import random
            selected_prompt = random.choice(base_prompts)
            response += f"\n[IMAGE_PROMPT: {selected_prompt}]"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ –≤–æ–ø—Ä–æ—Å –≤ –∫–æ–Ω—Ü–µ
        if not self._has_conversation_hook(response):
            # –î–æ–±–∞–≤–ª—è–µ–º –≤–æ–ø—Ä–æ—Å –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –±–µ—Å–µ–¥—ã
            hooks = [
                " –ê —Ç—ã —á—Ç–æ –¥—É–º–∞–µ—à—å?",
                " –ö–∞–∫ —É —Ç–µ–±—è —Å —ç—Ç–∏–º?",
                " –ê —É —Ç–µ–±—è –∫–∞–∫ –¥–µ–ª–∞ —Å —ç—Ç–∏–º?",
                " –†–∞—Å—Å–∫–∞–∂–∏ —Å–≤–æ–µ –º–Ω–µ–Ω–∏–µ!",
                " –ü–æ–¥–µ–ª–∏—Å—å —Å–≤–æ–∏–º–∏ –º—ã—Å–ª—è–º–∏!",
                " –ß—Ç–æ —Å–∫–∞–∂–µ—à—å?"
            ]
            import random
            hook = random.choice(hooks)
            # –í—Å—Ç–∞–≤–ª—è–µ–º –ø–µ—Ä–µ–¥ –ø—Ä–æ–º–ø—Ç–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            if "[IMAGE_PROMPT:" in response:
                parts = response.split("[IMAGE_PROMPT:")
                response = parts[0].rstrip() + hook + "\n[IMAGE_PROMPT:" + parts[1]
            else:
                response += hook
        
        return response
    
    def _has_conversation_hook(self, response: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –µ—Å—Ç—å –ª–∏ –≤ –æ—Ç–≤–µ—Ç–µ —ç–ª–µ–º–µ–Ω—Ç—ã –¥–ª—è –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏—è –±–µ—Å–µ–¥—ã."""
        # –£–±–∏—Ä–∞–µ–º –ø—Ä–æ–º–ø—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        text_only = response.split("[IMAGE_PROMPT:")[0] if "[IMAGE_PROMPT:" in response else response
        
        hook_indicators = [
            "?", "—Ä–∞—Å—Å–∫–∞–∂–∏", "–ø–æ–¥–µ–ª–∏—Å—å", "—á—Ç–æ –¥—É–º–∞–µ—à—å", "–∫–∞–∫ —É —Ç–µ–±—è", 
            "–∞ —Ç—ã", "–¥–∞–≤–∞–π", "–º–æ–∂–µ—Ç", "–ø—Ä–µ–¥–ª–∞–≥–∞—é", "–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ", "—á—Ç–æ —Å–∫–∞–∂–µ—à—å"
        ]
        
        return any(indicator in text_only.lower() for indicator in hook_indicators)
    
    async def check_health(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Ä–æ–ª—å-–ø–ª–µ–π –∫–ª–∏–µ–Ω—Ç–∞."""
        if ollama is None:
            return False
            
        try:
            await asyncio.get_event_loop().run_in_executor(
                self._executor, ollama.list
            )
            return True
        except Exception:
            return False
    
    def get_available_models(self) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏."""
        return self._get_available_models_sync()
    
    def _get_available_models_sync(self) -> List[str]:
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –ø–æ–ª—É—á–µ–Ω–∏–µ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."""
        if ollama is None:
            return []
            
        try:
            models_response = ollama.list()
            
            if hasattr(models_response, 'models'):
                models_list = models_response.models
            elif isinstance(models_response, dict):
                models_list = models_response.get('models', [])
            else:
                return []
            
            models = []
            for model in models_list:
                if hasattr(model, 'model'):
                    models.append(model.model)
                elif isinstance(model, dict):
                    models.append(model.get('model', ''))
                elif hasattr(model, 'name'):
                    models.append(model.name)
            
            return [m for m in models if m]
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {e}")
            return []
    
    async def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ —Ä–æ–ª—å-–ø–ª–µ–π –∫–ª–∏–µ–Ω—Ç–∞."""
        logger.info("üßπ –û—á–∏—Å—Ç–∫–∞ Roleplay Ollama –∫–ª–∏–µ–Ω—Ç–∞...")
        if hasattr(self, '_executor'):
            self._executor.shutdown(wait=True)
        logger.debug("‚úÖ Roleplay Ollama –∫–ª–∏–µ–Ω—Ç –æ—á–∏—â–µ–Ω")
    
    def get_roleplay_stats(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–æ–ª—å-–ø–ª–µ–π –∫–ª–∏–µ–Ω—Ç–∞."""
        return {
            "model": self.active_model,
            "model_type": self.model_type,
            "available": self.is_available,
            "temperature": self.roleplay_settings["temperature"],
            "max_tokens": self.roleplay_settings["max_tokens"],
            "optimized_for": f"roleplay_and_image_generation_{self.model_type}",
            "dolphin_optimized": self.model_type == "dolphin"
        }
    
    def __del__(self):
        """–î–µ—Å—Ç—Ä—É–∫—Ç–æ—Ä –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ executor."""
        if hasattr(self, '_executor'):
            try:
                self._executor.shutdown(wait=False)
            except Exception:
                pass