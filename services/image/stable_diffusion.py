"""–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ Stable Diffusion - –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è —Å –ø—Å–µ–≤–¥–æ–Ω–∏–º–æ–º."""

import asyncio
import time
import uuid
import logging
from pathlib import Path
from typing import List, Optional

from services.image.base_generator import BaseImageGenerator, ImagePrompt, GeneratedImage

logger = logging.getLogger(__name__)

class StableDiffusionGenerator(BaseImageGenerator):
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –Ω–∞ –æ—Å–Ω–æ–≤–µ Stable Diffusion."""
    
    def __init__(self, model_path: str, output_dir: str, **kwargs):
        super().__init__(model_path, output_dir, **kwargs)
        self.pipe = None
        self.device = kwargs.get('device', 'cpu')
    
    async def initialize(self) -> bool:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä."""
        try:
            logger.info("üé® –ù–∞—á–∞–ª–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Stable Diffusion...")
            
            # –õ–µ–Ω–∏–≤—ã–π –∏–º–ø–æ—Ä—Ç –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
            from diffusers import StableDiffusionPipeline
            import torch
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            if self.device == 'auto':
                self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
            
            logger.info(f"üîß –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å {self.model_path} –Ω–∞ {self.device}...")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            self.pipe = StableDiffusionPipeline.from_pretrained(
                self.model_path,
                torch_dtype=torch.float16 if self.device == 'cuda' else torch.float32,
                safety_checker=None if not self.config.get('safety_check', True) else None
            )
            
            self.pipe = self.pipe.to(self.device)
            
            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è CPU/GPU
            if self.device == 'cuda':
                self.pipe.enable_memory_efficient_attention()
                logger.info("‚úÖ –í–∫–ª—é—á–µ–Ω–∞ memory efficient attention –¥–ª—è GPU")
            else:
                # –î–ª—è CPU –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –ø—Ä–æ—Å—Ç—É—é –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é
                try:
                    self.pipe.enable_sequential_cpu_offload()
                    logger.info("‚úÖ –í–∫–ª—é—á–µ–Ω sequential CPU offload")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –≤–∫–ª—é—á–∏—Ç—å CPU offload: {e}")
                    logger.info("üí° –†–∞–±–æ—Ç–∞–µ–º –±–µ–∑ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ CPU (–¥–ª—è –≤–∫–ª—é—á–µ–Ω–∏—è —É—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install accelerate)")
            
            # –°–æ–∑–¥–∞–µ–º –≤—ã—Ö–æ–¥–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            self.output_dir.mkdir(parents=True, exist_ok=True)
            
            self.is_initialized = True
            logger.info(f"‚úÖ Stable Diffusion –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–æ–≤–∞–Ω –Ω–∞ {self.device}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Stable Diffusion: {e}")
            self.is_initialized = False
            return False
    
    async def generate(self, prompt: ImagePrompt) -> GeneratedImage:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ."""
        if not self.is_initialized:
            raise RuntimeError("–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        if not self.validate_prompt(prompt):
            raise ValueError("–ù–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–π –ø—Ä–æ–º–ø—Ç")
        
        start_time = time.time()
        
        try:
            logger.info(f"üé® –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: '{prompt.text[:50]}...'")
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
            image = await asyncio.get_event_loop().run_in_executor(
                None, self._generate_sync, prompt
            )
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image_path = self._save_image(image, prompt)
            
            generation_time = time.time() - start_time
            logger.info(f"‚úÖ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–æ –∑–∞ {generation_time:.1f}—Å")
            
            return GeneratedImage(
                image_path=image_path,
                prompt=prompt,
                metadata={
                    "model": self.model_path,
                    "device": self.device,
                    "seed": prompt.seed
                },
                generation_time=generation_time
            )
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            raise
    
    def get_available_styles(self) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Å—Ç–∏–ª–∏."""
        return [
            "realistic", "anime", "cartoon", "oil_painting",
            "watercolor", "sketch", "digital_art", "photographic"
        ]
    
    def _generate_sync(self, prompt: ImagePrompt):
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è."""
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        full_prompt = self._build_full_prompt(prompt)
        
        logger.debug(f"–ü–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç: {full_prompt}")
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º
        result = self.pipe(
            prompt=full_prompt,
            negative_prompt=prompt.negative_prompt,
            width=prompt.size[0],
            height=prompt.size[1],
            num_inference_steps=prompt.steps,
            guidance_scale=prompt.cfg_scale,
            generator=self._get_generator(prompt.seed)
        )
        
        return result.images[0]
    
    def _build_full_prompt(self, prompt: ImagePrompt) -> str:
        """–°—Ç—Ä–æ–∏—Ç –ø–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç —Å —É—á–µ—Ç–æ–º —Å—Ç–∏–ª—è."""
        parts = [prompt.text]
        
        if prompt.style:
            style_modifiers = {
                "realistic": "photorealistic, high quality, detailed",
                "anime": "anime style, manga, japanese animation",
                "cartoon": "cartoon style, animated, colorful",
                "oil_painting": "oil painting, classical art, brushstrokes",
                "watercolor": "watercolor painting, soft colors, artistic",
                "sketch": "pencil sketch, black and white, artistic drawing",
                "digital_art": "digital art, modern, clean lines",
                "photographic": "photograph, camera shot, professional"
            }
            
            if prompt.style in style_modifiers:
                parts.append(style_modifiers[prompt.style])
        
        return ", ".join(parts)
    
    def _get_generator(self, seed: Optional[int]):
        """–°–æ–∑–¥–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å seed."""
        import torch
        if seed is not None:
            return torch.Generator(device=self.device).manual_seed(seed)
        return None
    
    def _save_image(self, image, prompt: ImagePrompt) -> Path:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ."""
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞
        timestamp = int(time.time())
        unique_id = str(uuid.uuid4())[:8]
        filename = f"img_{timestamp}_{unique_id}.png"
        
        image_path = self.output_dir / filename
        image.save(image_path)
        
        logger.info(f"üíæ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {image_path}")
        return image_path
    
    async def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤."""
        logger.info("üßπ –û—á–∏—Å—Ç–∫–∞ Stable Diffusion...")
        if self.pipe is not None:
            # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º GPU –ø–∞–º—è—Ç—å
            if hasattr(self.pipe, 'to'):
                self.pipe.to('cpu')
            del self.pipe
            self.pipe = None
            
            # –û—á–∏—â–∞–µ–º CUDA –∫–µ—à –µ—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPU
            try:
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            except:
                pass
        
        self.is_initialized = False
        logger.debug("‚úÖ Stable Diffusion –æ—á–∏—â–µ–Ω")


class EnhancedStableDiffusionGenerator(BaseImageGenerator):
    """–£–ª—É—á—à–µ–Ω–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∫–∞—Å—Ç–æ–º–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π."""
    
    def __init__(self, model_path: str, output_dir: str, **kwargs):
        super().__init__(model_path, output_dir, **kwargs)
        self.pipe = None
        self.device = kwargs.get('device', 'auto')
        self.custom_model_configs = {
            'one-obsession': {
                'repo_id': 'stablediffusionapi/one-obsession-12-2-3d-details',
                'fallback': 'runwayml/stable-diffusion-v1-5',
                'optimal_steps': 25,
                'optimal_cfg': 8.0,
                'style_prefix': '3D detailed, high quality, obsession style',
                'negative_base': 'low quality, blurry, distorted, ugly, bad anatomy'
            },
            'dolphin-style': {
                'style_prefix': 'dolphin art style, creative, artistic',
                'optimal_steps': 20,
                'optimal_cfg': 7.5
            }
        }
    
    async def initialize(self) -> bool:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–æ–¥–µ–ª—å—é."""
        try:
            logger.info(f"üé® –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞...")
            logger.info(f"üéØ –¶–µ–ª–µ–≤–∞—è –º–æ–¥–µ–ª—å: {self.model_path}")
            
            # –õ–µ–Ω–∏–≤—ã–π –∏–º–ø–æ—Ä—Ç
            from diffusers import StableDiffusionPipeline
            import torch
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            if self.device == 'auto':
                self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
            
            # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–∞—Å—Ç–æ–º–Ω—É—é –º–æ–¥–µ–ª—å
            model_loaded = await self._try_load_custom_model()
            
            if not model_loaded:
                logger.warning("‚ö†Ô∏è –ö–∞—Å—Ç–æ–º–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                model_loaded = await self._load_fallback_model()
            
            if model_loaded:
                self._setup_optimizations()
                self.output_dir.mkdir(parents=True, exist_ok=True)
                self.is_initialized = True
                logger.info(f"‚úÖ –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –Ω–∞ {self.device}")
                return True
            else:
                logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–∏ –æ–¥–Ω—É –º–æ–¥–µ–ª—å")
                return False
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞: {e}")
            self.is_initialized = False
            return False
    
    async def _try_load_custom_model(self) -> bool:
        """–ü—ã—Ç–∞–µ—Ç—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–∞—Å—Ç–æ–º–Ω—É—é –º–æ–¥–µ–ª—å."""
        try:
            from diffusers import StableDiffusionPipeline
            import torch
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏–∑–≤–µ—Å—Ç–Ω—ã–µ –∫–∞—Å—Ç–æ–º–Ω—ã–µ –º–æ–¥–µ–ª–∏
            model_config = None
            model_repo = self.model_path
            
            for key, config in self.custom_model_configs.items():
                if key in self.model_path.lower() or config.get('repo_id') == self.model_path:
                    model_config = config
                    model_repo = config.get('repo_id', self.model_path)
                    break
            
            logger.info(f"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–∞—Å—Ç–æ–º–Ω—É—é –º–æ–¥–µ–ª—å: {model_repo}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            self.pipe = StableDiffusionPipeline.from_pretrained(
                model_repo,
                torch_dtype=torch.float16 if self.device == 'cuda' else torch.float32,
                safety_checker=None if not self.config.get('safety_check', True) else None,
                use_safetensors=True,
                variant="fp16" if self.device == 'cuda' else None
            )
            
            self.pipe = self.pipe.to(self.device)
            self.current_model_config = model_config
            
            logger.info(f"‚úÖ –ö–∞—Å—Ç–æ–º–Ω–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_repo}")
            return True
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∫–∞—Å—Ç–æ–º–Ω—É—é –º–æ–¥–µ–ª—å: {e}")
            return False
    
    async def _load_fallback_model(self) -> bool:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç fallback –º–æ–¥–µ–ª—å."""
        try:
            from diffusers import StableDiffusionPipeline
            import torch
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º fallback –º–æ–¥–µ–ª—å
            fallback_model = "runwayml/stable-diffusion-v1-5"
            
            if hasattr(self, 'current_model_config') and self.current_model_config:
                fallback_model = self.current_model_config.get('fallback', fallback_model)
            
            logger.info(f"üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º fallback –º–æ–¥–µ–ª—å: {fallback_model}")
            
            self.pipe = StableDiffusionPipeline.from_pretrained(
                fallback_model,
                torch_dtype=torch.float16 if self.device == 'cuda' else torch.float32,
                safety_checker=None if not self.config.get('safety_check', True) else None
            )
            
            self.pipe = self.pipe.to(self.device)
            
            logger.info(f"‚úÖ Fallback –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {fallback_model}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ fallback –º–æ–¥–µ–ª–∏: {e}")
            return False
    
    def _setup_optimizations(self):
        """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞."""
        try:
            if self.device == 'cuda':
                # GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
                if hasattr(self.pipe, 'enable_memory_efficient_attention'):
                    self.pipe.enable_memory_efficient_attention()
                    logger.info("‚úÖ Memory efficient attention –≤–∫–ª—é—á–µ–Ω")
                
                if hasattr(self.pipe, 'enable_xformers_memory_efficient_attention'):
                    try:
                        self.pipe.enable_xformers_memory_efficient_attention()
                        logger.info("‚úÖ XFormers attention –≤–∫–ª—é—á–µ–Ω")
                    except Exception:
                        logger.info("üí° XFormers –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π attention")
            else:
                # CPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
                try:
                    if hasattr(self.pipe, 'enable_sequential_cpu_offload'):
                        self.pipe.enable_sequential_cpu_offload()
                        logger.info("‚úÖ Sequential CPU offload –≤–∫–ª—é—á–µ–Ω")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è CPU offload –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
                    
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è –ù–µ–∫–æ—Ç–æ—Ä—ã–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã: {e}")
    
    async def generate(self, prompt: ImagePrompt) -> GeneratedImage:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏."""
        if not self.is_initialized:
            raise RuntimeError("–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        
        if not self.validate_prompt(prompt):
            raise ValueError("–ù–µ–¥–æ–ø—É—Å—Ç–∏–º—ã–π –ø—Ä–æ–º–ø—Ç")
        
        start_time = time.time()
        
        try:
            logger.info(f"üé® –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏: '{prompt.text[:50]}...'")
            
            # –ü—Ä–∏–º–µ–Ω—è–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
            enhanced_prompt = self._enhance_prompt_for_model(prompt)
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image = await asyncio.get_event_loop().run_in_executor(
                None, self._generate_sync, enhanced_prompt
            )
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º
            image_path = self._save_image(image, enhanced_prompt)
            
            generation_time = time.time() - start_time
            logger.info(f"‚úÖ –ö–∞—Å—Ç–æ–º–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ–∑–¥–∞–Ω–æ –∑–∞ {generation_time:.1f}—Å")
            
            return GeneratedImage(
                image_path=image_path,
                prompt=enhanced_prompt,
                metadata={
                    "model": self.model_path,
                    "device": self.device,
                    "custom_config": getattr(self, 'current_model_config', None),
                    "generation_time": generation_time
                },
                generation_time=generation_time
            )
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            raise
    
    def _enhance_prompt_for_model(self, prompt: ImagePrompt) -> ImagePrompt:
        """–£–ª—É—á—à–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏."""
        enhanced_text = prompt.text
        enhanced_negative = prompt.negative_prompt or ""
        
        # –ü—Ä–∏–º–µ–Ω—è–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∫–∞—Å—Ç–æ–º–Ω–æ–π –º–æ–¥–µ–ª–∏
        if hasattr(self, 'current_model_config') and self.current_model_config:
            config = self.current_model_config
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å —Å—Ç–∏–ª—è
            style_prefix = config.get('style_prefix', '')
            if style_prefix:
                enhanced_text = f"{style_prefix}, {enhanced_text}"
            
            # –£–ª—É—á—à–∞–µ–º –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            negative_base = config.get('negative_base', '')
            if negative_base:
                if enhanced_negative:
                    enhanced_negative = f"{negative_base}, {enhanced_negative}"
                else:
                    enhanced_negative = negative_base
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            steps = config.get('optimal_steps', prompt.steps)
            cfg_scale = config.get('optimal_cfg', prompt.cfg_scale)
        else:
            steps = prompt.steps
            cfg_scale = prompt.cfg_scale
        
        return ImagePrompt(
            text=enhanced_text,
            negative_prompt=enhanced_negative,
            style=prompt.style,
            size=prompt.size,
            steps=steps,
            cfg_scale=cfg_scale,
            seed=prompt.seed
        )
    
    def _generate_sync(self, prompt: ImagePrompt):
        """–°–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å –∫–∞—Å—Ç–æ–º–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏."""
        logger.debug(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è: {prompt.text[:100]}...")
        logger.debug(f"–ù–µ–≥–∞—Ç–∏–≤: {prompt.negative_prompt[:100] if prompt.negative_prompt else '–ù–µ—Ç'}")
        logger.debug(f"–®–∞–≥–∏: {prompt.steps}, CFG: {prompt.cfg_scale}")
        
        result = self.pipe(
            prompt=prompt.text,
            negative_prompt=prompt.negative_prompt,
            width=prompt.size[0],
            height=prompt.size[1],
            num_inference_steps=prompt.steps,
            guidance_scale=prompt.cfg_scale,
            generator=self._get_generator(prompt.seed)
        )
        
        return result.images[0]
    
    def _get_generator(self, seed: Optional[int]):
        """–°–æ–∑–¥–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å seed."""
        import torch
        if seed is not None:
            return torch.Generator(device=self.device).manual_seed(seed)
        return None
    
    def _save_image(self, image, prompt: ImagePrompt) -> Path:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –º–µ—Ç–∫–æ–π –º–æ–¥–µ–ª–∏."""
        timestamp = int(time.time())
        unique_id = str(uuid.uuid4())[:8]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª–∏ –≤ –∏–º—è —Ñ–∞–π–ª–∞
        model_tag = "custom" if hasattr(self, 'current_model_config') else "standard"
        filename = f"img_{model_tag}_{timestamp}_{unique_id}.png"
        
        image_path = self.output_dir / filename
        image.save(image_path)
        
        logger.info(f"üíæ –ö–∞—Å—Ç–æ–º–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {image_path}")
        return image_path
    
    def get_available_styles(self) -> List[str]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Å—Ç–∏–ª–∏ –≤–∫–ª—é—á–∞—è –∫–∞—Å—Ç–æ–º–Ω—ã–µ."""
        base_styles = [
            "realistic", "anime", "cartoon", "oil_painting",
            "watercolor", "sketch", "digital_art", "photographic"
        ]
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∫–∞—Å—Ç–æ–º–Ω—ã–µ —Å—Ç–∏–ª–∏
        if hasattr(self, 'current_model_config') and self.current_model_config:
            base_styles.extend([
                "3d_detailed", "obsession_style", "high_contrast",
                "cinematic", "dramatic_lighting"
            ])
        
        return base_styles
    
    async def cleanup(self):
        """–û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞."""
        logger.info("üßπ –û—á–∏—Å—Ç–∫–∞ –∫–∞—Å—Ç–æ–º–Ω–æ–≥–æ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞...")
        if self.pipe is not None:
            if hasattr(self.pipe, 'to'):
                self.pipe.to('cpu')
            del self.pipe
            self.pipe = None
            
            try:
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
            except:
                pass
        
        self.is_initialized = False
        logger.debug("‚úÖ –ö–∞—Å—Ç–æ–º–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –æ—á–∏—â–µ–Ω")

# –î–æ–±–∞–≤—å –≤ –∫–æ–Ω–µ—Ü —Ñ–∞–π–ª–∞ services/image/stable_diffusion.py:

class LocalStableDiffusionGenerator(StableDiffusionGenerator):
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–ª—è –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (.safetensors/.ckpt —Ñ–∞–π–ª–æ–≤)."""
    
    async def initialize(self) -> bool:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Å –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª—å—é."""
        try:
            logger.info(f"üé® –ó–∞–≥—Ä—É–∑–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {self.model_path}")
            
            from diffusers import StableDiffusionPipeline
            import torch
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
            if self.device == 'auto':
                self.device = 'cuda' if torch.cuda.is_available() else 'cpu'
            
            logger.info(f"üìÅ –ó–∞–≥—Ä—É–∂–∞–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª –Ω–∞ {self.device}...")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑ single file
            self.pipe = StableDiffusionPipeline.from_single_file(
                self.model_path,
                torch_dtype=torch.float16 if self.device == 'cuda' else torch.float32,
                safety_checker=None if not self.config.get('safety_check', True) else None,
                use_safetensors=self.model_path.endswith('.safetensors'),
                load_safety_checker=False
            )
            
            self.pipe = self.pipe.to(self.device)
            
            # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è CPU/GPU
            if self.device == 'cuda':
                try:
                    self.pipe.enable_memory_efficient_attention()
                    logger.info("‚úÖ Memory efficient attention –≤–∫–ª—é—á–µ–Ω")
                except:
                    logger.info("üí° Memory efficient attention –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            else:
                try:
                    self.pipe.enable_sequential_cpu_offload()
                    logger.info("‚úÖ Sequential CPU offload –≤–∫–ª—é—á–µ–Ω")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è CPU offload –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
            
            # –°–æ–∑–¥–∞–µ–º –≤—ã—Ö–æ–¥–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
            self.output_dir.mkdir(parents=True, exist_ok=True)
            
            self.is_initialized = True
            logger.info(f"‚úÖ –õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å One Obsession –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {self.device}")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {e}")
            logger.info("üí° –ü—Ä–æ–≤–µ—Ä—å –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –∏ —Ñ–æ—Ä–º–∞—Ç –º–æ–¥–µ–ª–∏")
            self.is_initialized = False
            return False
    
    def _build_full_prompt(self, prompt: ImagePrompt) -> str:
        """–°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–º–ø—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –¥–ª—è One Obsession –º–æ–¥–µ–ª–∏."""
        parts = []
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Å—Ç–∏–ª–µ–≤—ã–µ –ø—Ä–µ—Ñ–∏–∫—Å—ã –¥–ª—è One Obsession
        style_prefix = "3D detailed, high quality, obsession style"
        parts.append(style_prefix)
        parts.append(prompt.text)
        
        if prompt.style:
            style_modifiers = {
                "realistic": "photorealistic, highly detailed, professional",
                "anime": "anime style, detailed character design",
                "cartoon": "cartoon style, vibrant colors, detailed",
                "oil_painting": "oil painting style, artistic, detailed brushwork",
                "watercolor": "watercolor style, soft artistic colors",
                "sketch": "detailed sketch, artistic drawing",
                "digital_art": "digital art, modern style, highly detailed",
                "photographic": "photograph quality, professional lighting"
            }
            
            if prompt.style in style_modifiers:
                parts.append(style_modifiers[prompt.style])
        
        return ", ".join(parts)
    
    def _save_image(self, image, prompt: ImagePrompt) -> Path:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å –º–µ—Ç–∫–æ–π One Obsession."""
        timestamp = int(time.time())
        unique_id = str(uuid.uuid4())[:8]
        filename = f"img_oneobsession_{timestamp}_{unique_id}.png"
        
        image_path = self.output_dir / filename
        image.save(image_path)
        
        logger.info(f"üíæ One Obsession –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {image_path}")
        return image_path