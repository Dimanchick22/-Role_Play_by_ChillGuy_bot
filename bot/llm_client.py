"""–ö–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å LLM —á–µ—Ä–µ–∑ Ollama."""

import logging
import random
import time
from typing import Dict, List, Optional

import ollama

logger = logging.getLogger(__name__)


class LLMClient:
    """–ö–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –ª–æ–∫–∞–ª—å–Ω–æ–π LLM."""
    
    def __init__(self, model_name: str, max_history: int = 10):
        self.model_name = model_name
        self.max_history = max_history
        self.conversations: Dict[int, List[Dict]] = {}
        self.system_prompt = ""
        
        # –ï—Å–ª–∏ model_name —ç—Ç–æ "auto", –Ω–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ä–∞–∑—É
        if model_name != "auto":
            self._verify_model()
    
    def set_model(self, model_name: str) -> None:
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å."""
        self.model_name = model_name
        self._verify_model()
        logger.info(f"–ú–æ–¥–µ–ª—å –∏–∑–º–µ–Ω–µ–Ω–∞ –Ω–∞: {model_name}")
    
    def _verify_model(self) -> None:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏."""
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
            models_response = ollama.list()
            available_models = []
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã –æ—Ç–≤–µ—Ç–∞
            if hasattr(models_response, 'models'):
                models_list = models_response.models
            elif isinstance(models_response, dict) and 'models' in models_response:
                models_list = models_response['models']
            else:
                logger.warning(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞: {type(models_response)}")
                models_list = []
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º–µ–Ω–∞ –º–æ–¥–µ–ª–µ–π
            for model in models_list:
                if hasattr(model, 'model'):
                    available_models.append(model.model)
                elif isinstance(model, dict) and 'model' in model:
                    available_models.append(model['model'])
            
            logger.info(f"–î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {available_models}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞—à—É –º–æ–¥–µ–ª—å
            if self.model_name not in available_models:
                logger.warning(f"–ú–æ–¥–µ–ª—å {self.model_name} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
                
                # –ò—â–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É
                alternative = self._find_alternative_model(available_models)
                if alternative:
                    self.model_name = alternative
                    logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å: {self.model_name}")
                else:
                    raise RuntimeError("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º –º–æ–¥–µ–ª—å
            self._test_model()
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            raise
    
    def _find_alternative_model(self, available: List[str]) -> Optional[str]:
        """–ò—â–µ—Ç –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å."""
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç –º–æ–¥–µ–ª–µ–π
        preferred = ['llama3.2:3b', 'llama3.2', 'mistral', 'qwen']
        
        for pref in preferred:
            for model in available:
                if pref in model.lower():
                    return model
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—É—é –¥–æ—Å—Ç—É–ø–Ω—É—é
        return available[0] if available else None
    
    def _test_model(self) -> None:
        """–¢–µ—Å—Ç–∏—Ä—É–µ—Ç –º–æ–¥–µ–ª—å –ø—Ä–æ—Å—Ç—ã–º –∑–∞–ø—Ä–æ—Å–æ–º."""
        try:
            response = ollama.generate(
                model=self.model_name,
                prompt="–°–∫–∞–∂–∏ '—Ç–µ—Å—Ç' –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ",
                options={'num_predict': 5}
            )
            logger.info(f"–ú–æ–¥–µ–ª—å {self.model_name} —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
            raise
    
    def set_system_prompt(self, prompt: str) -> None:
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç —Å–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç."""
        self.system_prompt = prompt
        logger.debug("–°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    
    def _get_history(self, user_id: int) -> List[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é —Ä–∞–∑–≥–æ–≤–æ—Ä–∞."""
        return self.conversations.get(user_id, [])
    
    def _add_to_history(self, user_id: int, role: str, content: str) -> None:
        """–î–æ–±–∞–≤–ª—è–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏–µ –≤ –∏—Å—Ç–æ—Ä–∏—é."""
        if user_id not in self.conversations:
            self.conversations[user_id] = []
        
        self.conversations[user_id].append({"role": role, "content": content})
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∏—Å—Ç–æ—Ä–∏–∏
        if len(self.conversations[user_id]) > self.max_history * 2:
            self.conversations[user_id] = self.conversations[user_id][-self.max_history * 2:]
    
    async def get_response(self, message: str, user_id: int) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç –æ—Ç–≤–µ—Ç –æ—Ç LLM."""
        try:
            start_time = time.time()
            
            # –ì–æ—Ç–æ–≤–∏–º —Å–æ–æ–±—â–µ–Ω–∏—è
            messages = self._prepare_messages(user_id, message)
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å
            response = await self._call_llm(messages)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∏—Å—Ç–æ—Ä–∏—é
            self._add_to_history(user_id, "user", message)
            self._add_to_history(user_id, "assistant", response)
            
            # –õ–æ–≥–∏—Ä—É–µ–º
            elapsed = time.time() - start_time
            logger.info(f"–û—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω –∑–∞ {elapsed:.2f}—Å: {response[:50]}...")
            
            return response
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞: {e}")
            return self._get_fallback_response()
    
    def _prepare_messages(self, user_id: int, message: str) -> List[Dict]:
        """–ì–æ—Ç–æ–≤–∏—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è LLM."""
        messages = []
        
        # –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        if self.system_prompt:
            messages.append({"role": "system", "content": self.system_prompt})
        
        # –ò—Å—Ç–æ—Ä–∏—è
        messages.extend(self._get_history(user_id))
        
        # –¢–µ–∫—É—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        messages.append({"role": "user", "content": message})
        
        return messages
    
    async def _call_llm(self, messages: List[Dict]) -> str:
        """–í—ã–∑—ã–≤–∞–µ—Ç LLM API."""
        try:
            # –ü—Ä–æ–±—É–µ–º chat API
            response = ollama.chat(
                model=self.model_name,
                messages=messages,
                options={
                    'temperature': 0.7,
                    'top_p': 0.9,
                    'num_predict': 200
                }
            )
            return response['message']['content'].strip()
            
        except Exception:
            # Fallback –Ω–∞ generate API
            logger.info("–ü–µ—Ä–µ–∫–ª—é—á–∞–µ–º—Å—è –Ω–∞ generate API")
            prompt = self._messages_to_prompt(messages)
            
            response = ollama.generate(
                model=self.model_name,
                prompt=prompt,
                options={
                    'temperature': 0.7,
                    'top_p': 0.9, 
                    'num_predict': 200
                }
            )
            return response['response'].strip()
    
    def _messages_to_prompt(self, messages: List[Dict]) -> str:
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –≤ –ø—Ä–æ–º–ø—Ç."""
        prompt = ""
        for msg in messages:
            role = msg['role']
            content = msg['content']
            
            if role == 'system':
                prompt += f"–°–∏—Å—Ç–µ–º–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ: {content}\n\n"
            elif role == 'user':
                prompt += f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {content}\n"
            elif role == 'assistant':
                prompt += f"–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç: {content}\n"
        
        prompt += "–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:"
        return prompt
    
    def _get_fallback_response(self) -> str:
        """–ó–∞–ø–∞—Å–Ω–æ–π –æ—Ç–≤–µ—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ."""
        responses = [
            "–£–ø—Å! üôà –ß—Ç–æ-—Ç–æ —Å –º–æ–∏–º–∏ –º—ã—Å–ª—è–º–∏... –ü–æ–ø—Ä–æ–±—É–π –µ—â–µ —Ä–∞–∑!",
            "–•–º, –∫–∞–∂–µ—Ç—Å—è —è –∑–∞–¥—É–º–∞–ª–∞—Å—å ü§î –ü–æ–≤—Ç–æ—Ä–∏, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞?",
            "–û–π, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ üòÖ –î–∞–≤–∞–π –µ—â–µ —Ä–∞–∑!",
            "–ú–æ—è –Ω–µ–π—Ä–æ—Å–µ—Ç—å –ø–æ–¥–≤–∏—Å–ª–∞ ü§ñ –ü–æ–ø—Ä–æ–±—É–π —Å–Ω–æ–≤–∞!"
        ]
        return random.choice(responses)
    
    def clear_history(self, user_id: int) -> None:
        """–û—á–∏—â–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—é –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
        if user_id in self.conversations:
            del self.conversations[user_id]
            logger.info(f"–ò—Å—Ç–æ—Ä–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id} –æ—á–∏—â–µ–Ω–∞")
    
    def get_stats(self) -> Dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ä–∞–±–æ—Ç—ã."""
        return {
            'model': self.model_name,
            'active_conversations': len(self.conversations),
            'max_history': self.max_history
        }