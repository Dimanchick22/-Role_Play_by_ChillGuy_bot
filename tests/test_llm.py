"""–¢–µ—Å—Ç—ã –¥–ª—è LLM –∫–ª–∏–µ–Ω—Ç–∞."""

import asyncio
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from bot.character import Character
from bot.llm_client import LLMClient


def test_ollama_connection():
    """–¢–µ—Å—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama."""
    print("=== –¢–µ—Å—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ Ollama ===")
    
    try:
        import ollama
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π
        models_response = ollama.list()
        print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Ollama —É—Å–ø–µ—à–Ω–æ!")
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤ –æ—Ç–≤–µ—Ç–∞
        if hasattr(models_response, 'models'):
            models_list = models_response.models
        elif isinstance(models_response, dict) and 'models' in models_response:
            models_list = models_response['models']
        else:
            print(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞: {type(models_response)}")
            return False
        
        print(f"–ù–∞–π–¥–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(models_list)}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3 –º–æ–¥–µ–ª–∏
        for i, model in enumerate(models_list[:3]):
            if hasattr(model, 'model'):
                name = model.model
            elif isinstance(model, dict) and 'model' in model:
                name = model['model']
            else:
                name = str(model)
            print(f"  {i+1}. {name}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: {e}")
        print("\n–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
        print("1. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Ollama: curl -fsSL https://ollama.com/install.sh | sh")
        print("2. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–µ—Ä–≤–µ—Ä: ollama serve")
        print("3. –°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å: ollama pull llama3.2:3b")
        return False


async def test_llm_client():
    """–¢–µ—Å—Ç LLM –∫–ª–∏–µ–Ω—Ç–∞."""
    print("\n=== –¢–µ—Å—Ç LLM –∫–ª–∏–µ–Ω—Ç–∞ ===")
    
    try:
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è
        character = Character()
        llm = LLMClient("llama3.2:3b", max_history=5)
        
        print(f"‚úÖ LLM –∫–ª–∏–µ–Ω—Ç —Å–æ–∑–¥–∞–Ω")
        print(f"–ú–æ–¥–µ–ª—å: {llm.model_name}")
        
        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø—Ä–æ–º–ø—Ç–∞
        prompt = character.get_system_prompt("–¢–µ—Å—Ç–µ—Ä")
        llm.set_system_prompt(prompt)
        print("‚úÖ –°–∏—Å—Ç–µ–º–Ω—ã–π –ø—Ä–æ–º–ø—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        
        # –¢–µ—Å—Ç–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        test_messages = [
            "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?",
            "–†–∞—Å—Å–∫–∞–∂–∏ –æ —Å–µ–±–µ –∫—Ä–∞—Ç–∫–æ",
            "–°–ø–∞—Å–∏–±–æ!"
        ]
        
        user_id = 12345
        
        print("\n--- –î–∏–∞–ª–æ–≥ —Å LLM ---")
        for i, message in enumerate(test_messages, 1):
            print(f"\n{i}. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {message}")
            
            try:
                response = await llm.get_response(message, user_id)
                print(f"   –ê–ª–∏—Å–∞: {response}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ—Ç–≤–µ—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º
                if any(char in response for char in '–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è'):
                    print("   ‚úÖ –û—Ç–≤–µ—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ")
                else:
                    print("   ‚ö†Ô∏è –í–æ–∑–º–æ–∂–Ω–æ –Ω–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ")
                    
            except Exception as e:
                print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
        
        # –¢–µ—Å—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
        print(f"\n--- –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ ---")
        stats = llm.get_stats()
        for key, value in stats.items():
            print(f"{key}: {value}")
        
        # –¢–µ—Å—Ç –æ—á–∏—Å—Ç–∫–∏ –∏—Å—Ç–æ—Ä–∏–∏
        llm.clear_history(user_id)
        print("‚úÖ –ò—Å—Ç–æ—Ä–∏—è –æ—á–∏—â–µ–Ω–∞")
        
        return True
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è LLM: {e}")
        return False


async def run_tests():
    """–ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤."""
    print("üß™ –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ LLM\n")
    
    # –¢–µ—Å—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
    if not test_ollama_connection():
        print("\n‚ùå –¢–µ—Å—Ç—ã –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –∏–∑-–∑–∞ –ø—Ä–æ–±–ª–µ–º —Å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ–º")
        return
    
    # –¢–µ—Å—Ç –∫–ª–∏–µ–Ω—Ç–∞
    success = await test_llm_client()
    
    if success:
        print("\n‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã LLM –ø—Ä–æ–π–¥–µ–Ω—ã!")
    else:
        print("\n‚ùå –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ç–µ—Å—Ç—ã –Ω–µ –ø—Ä–æ—à–ª–∏")


if __name__ == "__main__":
    asyncio.run(run_tests())